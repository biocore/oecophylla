from parser import (combine_profiles,
                    extract_level,
                    combine_kraken,
                    combine_bracken,
                    pandas2biom)


rule taxonomy_metaphlan2:
    """
    Runs MetaPhlan2 on a set of samples to create a joint taxonomic profile for
    input into HUMAnN2, based on the thinking that it is preferable to have a
    consistent Chocophlan reference database for the whole set of samples. This
    is especially true for shallowly sequenced samples.

    Going to do just R1 reads for now. Because of how I've split PE vs SE
    processing and naming, still will need to make a separate rule for PE.
    """
    input:
        forward = qc_dir + "{sample}/filtered/{sample}.R1.trimmed.filtered.fastq.gz",
        reverse = qc_dir + "{sample}/filtered/{sample}.R2.trimmed.filtered.fastq.gz"
    output:
        profile = taxonomy_dir + "{sample}/metaphlan2/{sample}.profile.txt"
        # extra output file:
        # {sample}.map.txt.gz if {map} is ON
    params:
        env = config['envs']['metaphlan2'],
        db = config['params']['metaphlan2']['db'],
        map = config['params']['metaphlan2']['map']
    threads:
        2
    log:
        taxonomy_dir + "logs/taxonomy_metaphlan2.sample_{sample}.log"
    benchmark:
        "benchmarks/taxonomy/taxonomy_metaphlan2.sample_{sample}.txt"
    run:
        with tempfile.TemporaryDirectory(dir=find_local_scratch(TMP_DIR_ROOT)) as temp_dir:
            shell("""
                  set +u; {params.env}; set -u

                  # get stem file path
                  stem={output.profile}
                  stem=${{stem%.profile.txt}}

                  # merge input files
                  zcat {input.forward} {input.reverse} > {temp_dir}/input.fastq

                  # run MetaPhlAn2 to generate taxonomic profile
                  metaphlan2.py {temp_dir}/input.fastq \
                    --input_type fastq \
                    --mpa_pkl {params.db}.pkl \
                    --bowtie2db {params.db} \
                    --nproc {threads} \
                    --tmp_dir {temp_dir} \
                    --bowtie2out {temp_dir}/map.tmp \
                    -o {output.profile} \
                    2> {log} 1>&2

                  # keep mapping file
                  if [[ "{params.map}" == "True" ]]
                  then
                    gzip -c {temp_dir}/map.tmp > $stem.map.txt.gz
                  fi
                  """)


rule taxonomy_combine_metaphlan2:
    """
    Combines per-sample MetaPhlAn2 output profiles into single OTU tables.
    """
    input:
        expand(taxonomy_dir + "{sample}/metaphlan2/{sample}.profile.txt",
               sample=samples)
    output:
        taxonomy_dir + "metaphlan2/combined_profile.biom"
        # extra output files:
        # combined_profile.{level}.biom foreach {levels}
    threads:
        1
    params:
        name2tid = config['params']['metaphlan2']['name2tid'],
        levels = config['params']['metaphlan2']['levels']
    log:
        taxonomy_dir + "logs/taxonomy_combine_metaphlan2.log"
    benchmark:
        "benchmarks/taxonomy/taxonomy_combine_metaphlan2.txt"
    run:
        table = combine_profiles(zip(samples, input))
        pandas2biom(output[0], table)
        name2tid = None
        if params['name2tid']:
            with open(params['name2tid'], 'r') as f:
                name2tid = dict(x.split('\t') for x in f.read().splitlines())
        for level in params['levels'].split(','):
            pandas2biom('%s/metaphlan2/combined_profile.%s.biom'
                        % (taxonomy_dir, level),
                        extract_level(table, level[0].lower(), delim='|',
                                      dic=name2tid))


rule metaphlan2:
    input:
        taxonomy_dir + "metaphlan2/combined_profile.biom"


rule taxonomy_kraken:
    """
    Runs Kraken with Bracken to construct taxonomic profiles.
    """
    input:
        forward = qc_dir + "{sample}/filtered/{sample}.R1.trimmed.filtered.fastq.gz",
        reverse = qc_dir + "{sample}/filtered/{sample}.R2.trimmed.filtered.fastq.gz"
    output:
        report = taxonomy_dir + "{sample}/kraken/{sample}.report.txt",
        profile = taxonomy_dir + "{sample}/kraken/{sample}.profile.txt"
        # extra output files:
        # {sample}.map.txt.gz if {map} is ON
        # {sample}.redist.{level}.txt foreach {levels}
    params:
        env = config['envs']['kraken'],
        db = config['params']['kraken']['db'],
        kmers = config['params']['bracken']['kmers'],
        levels = config['params']['kraken']['levels'],
        map = config['params']['kraken']['map']
    threads:
        12
    log:
        taxonomy_dir + "logs/taxonomy_kraken.sample_{sample}.log"
    benchmark:
        "benchmarks/taxonomy/taxonomy_kraken.sample_{sample}.txt"
    run:
        with tempfile.TemporaryDirectory(dir=find_local_scratch(TMP_DIR_ROOT)) as temp_dir:
            shell("""
                  set +u; {params.env}; set -u

                  # get stem file path
                  stem={output.report}
                  stem=${{stem%.report.txt}}

                  # run Kraken to align reads against reference genomes
                  kraken {input.forward} {input.reverse} \
                    --db {params.db} \
                    --paired \
                    --fastq-input \
                    --gzip-compressed \
                    --only-classified-output \
                    --threads {threads} \
                    1> {temp_dir}/map.tmp \
                    2> {log}

                  # generate hierarchical report
                  kraken-report {temp_dir}/map.tmp \
                    --db {params.db} \
                    1> {output.report} \
                    2>> {log}

                  # generate lineage to count table
                  kraken-mpa-report {temp_dir}/map.tmp \
                    --db {params.db} \
                    1> {output.profile} \
                    2>> {log}

                  # keep mapping file
                  if [[ "{params.map}" == "True" ]]
                  then
                    gzip -c {temp_dir}/map.tmp > $stem.map.txt.gz
                  fi

                  # run Bracken to re-estimate abundance at given rank
                  if [[ ! -z {params.levels} ]]
                  then
                    IFS=',' read -r -a levels <<< "{params.levels}"
                    for level in "${{levels[@]}}"
                    do
                      est_abundance.py -i {output.report} \
                        -k {params.kmers} \
                        -t 10 \
                        -l $(echo $level | head -c 1 | tr a-z A-Z) \
                        -o $stem.redist.$level.txt \
                        2>> {log} 1>&2
                      rm $stem.report_bracken.txt
                    done
                  fi
                  """)


rule taxonomy_kraken_combine_profiles:
    """
    Combines per-sample Kraken/Bracken output tables into single OTU tables.
    """
    input:
        expand(taxonomy_dir + "{sample}/kraken/{sample}.profile.txt",
               sample=samples)
    output:
        taxonomy_dir + "kraken/combined_profile.biom"
        # extra output files:
        # combined_redist.{level}.biom foreach {levels}
    params:
        levels = config['params']['kraken']['levels']
    log:
        taxonomy_dir + "logs/taxonomy_kraken_combine_profiles.log"
    benchmark:
        "benchmarks/taxonomy/taxonomy_kraken_combine_profiles.txt"
    run:
        pandas2biom(output[0], combine_profiles(zip(samples, input)))
        for level in params['levels'].split(','):
            redists = ['%s/%s/kraken/%s.redist.%s.txt'
                       % (taxonomy_dir, sample, sample, level)
                       for sample in samples]
            pandas2biom('%s/kraken/combined_redist.%s.biom'
                        % (taxonomy_dir, level),
                        combine_bracken(zip(samples, redists)))


rule kraken:
    input:
        taxonomy_dir + "kraken/combined_profile.biom"


rule taxonomy_centrifuge:
    """
    Runs Centrifuge with Bracken to construct taxonomic profiles.
    """
    input:
        forward = qc_dir + "{sample}/filtered/{sample}.R1.trimmed.filtered.fastq.gz",
        reverse = qc_dir + "{sample}/filtered/{sample}.R2.trimmed.filtered.fastq.gz"
    output:
        report = taxonomy_dir + "{sample}/centrifuge/{sample}.report.txt",
        profile = taxonomy_dir + "{sample}/centrifuge/{sample}.profile.txt"
        # extra output files:
        # {sample}.map.txt.gz if {map} is ON
        # {sample}.redist.{level}.txt foreach {levels}
    params:
        env = config['envs']['centrifuge'],
        db = config['params']['centrifuge']['db'],
        kmers = config['params']['bracken']['kmers'],
        levels = config['params']['centrifuge']['levels'],
        map = config['params']['centrifuge']['map']
    threads:
        12
    log:
        taxonomy_dir + "logs/taxonomy_centrifuge.sample_{sample}.log"
    benchmark:
        "benchmarks/taxonomy/taxonomy_centrifuge.sample_{sample}.txt"
    run:
        with tempfile.TemporaryDirectory(dir=find_local_scratch(TMP_DIR_ROOT)) as temp_dir:
            shell("""
                  set +u; {params.env}; set -u

                  # get stem file path
                  stem={output.report}
                  stem=${{stem%.report.txt}}

                  # run Centrifuge to align reads against reference genomes
                  centrifuge \
                    -1 {input.forward} \
                    -2 {input.reverse} \
                    -x {params.db} \
                    -p {threads} \
                    -S {temp_dir}/map.tmp \
                    --report-file {output.profile} \
                    2> {log} 1>&2

                  # generate Kraken-style hierarchical report
                  centrifuge-kreport {temp_dir}/map.tmp \
                    -x {params.db} \
                    1> {output.report} \
                    2>> {log}

                  # keep mapping file
                  if [[ "{params.map}" == "True" ]]
                  then
                    gzip -c {temp_dir}/map.tmp > $stem.map.txt.gz
                  fi
                  """)


rule taxonomy_centrifuge_combine_profiles:
    """
    Combines per-sample Centrifuge's Kraken-style report files into single OTU
    tables.
    """
    input:
        expand(taxonomy_dir + "{sample}/centrifuge/{sample}.report.txt",
               sample=samples)
    output:
        taxonomy_dir + "centrifuge/combined_profile.biom"
        # extra output files:
        # combined_profile.{level}.biom foreach {levels}
    params:
        levels = config['params']['centrifuge']['levels']
    log:
        taxonomy_dir + "logs/taxonomy_centrifuge_combine_profiles.log"
    benchmark:
        "benchmarks/taxonomy/taxonomy_centrifuge_combine_profiles.txt"
    run:
        # this is not a typo. Centrifuge produces Kraken-style reports.
        comb, lv2tids = combine_kraken(zip(samples, input))
        pandas2biom(output[0], comb)
        for level in params['levels'].split(','):
            pandas2biom('%s/centrifuge/combined_profile.%s.biom'
                        % (taxonomy_dir, level),
                        comb[comb.index.isin(lv2tids[level[0].upper()])])


rule centrifuge:
    input:
        taxonomy_dir + "centrifuge/combined_profile.biom"


rule taxonomy_shogun:
    """
    Runs SHOGUN with choice of aligner to construct taxonomic profiles.
    """
    input:
        forward = qc_dir + "{sample}/filtered/{sample}.R1.trimmed.filtered.fastq.gz",
        reverse = qc_dir + "{sample}/filtered/{sample}.R2.trimmed.filtered.fastq.gz"
    output:
        profile = taxonomy_dir + "{sample}/shogun/{sample}.profile.txt",
        # extra output files:
        # {sample}.{aligner}.{ext}.gz if {map} is ON
        # {sample}.redist.{level}.txt foreach {levels}
    params:
        env = config['envs']['shogun'],
        db = config['params']['shogun']['db'],
        aligner = config['params']['shogun']['aligner'],
        levels = config['params']['shogun']['levels'],
        map = config['params']['shogun']['map']
    threads:
        12
    log:
        taxonomy_dir + "logs/taxonomy_shogun.sample_{sample}.log"
    benchmark:
        "benchmarks/taxonomy/taxonomy_shogun.sample_{sample}.txt"
    run:
        aln2ext = {'utree': 'tsv', 'burst': 'b6', 'bowtie2': 'sam'}
        ext = aln2ext[params['aligner']]
        with tempfile.TemporaryDirectory(dir=find_local_scratch(TMP_DIR_ROOT)) as temp_dir:
            shell("""
                  set +u; {params.env}; set -u

                  # get stem file path
                  stem={output.profile}
                  stem=${{stem%.profile.txt}}

                  # interleave paired fastq's and convert to fasta
                  seqtk mergepe {input.forward} {input.reverse} | \
                  seqtk seq -A > {temp_dir}/{wildcards.sample}.fna

                  # map reads to reference database
                  shogun align \
                  --aligner {params.aligner} \
                  --threads {threads} \
                  --database {params.db} \
                  --input {temp_dir}/{wildcards.sample}.fna \
                  --output {temp_dir} \
                  2> {log} 1>&2

                  # build taxonomic profile based on read map
                  shogun assign_taxonomy \
                  --aligner {params.aligner} \
                  --database {params.db} \
                  --input {temp_dir}/alignment.{params.aligner}.{ext} \
                  --output {output.profile} \
                  2> {log} 1>&2

                  # keep mapping file
                  if [[ "{params.map}" == "True" ]]
                  then
                    gzip -c {temp_dir}/alignment.{params.aligner}.{ext} > $stem.{params.aligner}.{ext}.gz
                  fi

                  # redistribute reads to given taxonomic ranks
                  if [[ ! -z {params.levels} ]]
                  then
                    IFS=',' read -r -a levels <<< "{params.levels}"
                    for level in "${{levels[@]}}"
                    do
                      shogun redistribute \
                      --database {params.db} \
                      --level $level \
                      --input {output.profile} \
                      --output $stem.redist.$level.txt \
                      2> {log} 1>&2
                    done
                  fi
                  """)


rule taxonomy_shogun_combine_profiles:
    """
    Combines per-sample SHOGUN output tables into single OTU tables.
    """
    input:
        expand(taxonomy_dir + "{sample}/shogun/{sample}.profile.txt",
               sample=samples)
    output:
        taxonomy_dir + "shogun/combined_profile.biom"
        # extra output files:
        # combined_redist.{level}.biom foreach {levels}
    params:
        levels = config['params']['shogun']['levels']
    log:
        taxonomy_dir + "logs/taxonomy_shogun_combine_profiles.log"
    benchmark:
        "benchmarks/taxonomy/taxonomy_shogun_combine_profiles.txt"
    run:
        pandas2biom(output[0], combine_profiles(zip(samples, input)))
        for level in params['levels'].split(','):
            redists = ['%s/%s/shogun/%s.redist.%s.txt'
                       % (taxonomy_dir, sample, sample, level)
                       for sample in samples]
            pandas2biom('%s/shogun/combined_redist.%s.biom'
                        % (taxonomy_dir, level),
                        combine_profiles(zip(samples, redists)))


rule shogun:
    input:
        taxonomy_dir + "shogun/combined_profile.biom"


rule taxonomy:
    input:
        taxonomy_dir + "metaphlan2/combined_profile.biom",
        taxonomy_dir + "kraken/combined_profile.biom",
        taxonomy_dir + "centrifuge/combined_profile.biom",
        taxonomy_dir + "shogun/combined_profile.biom"
